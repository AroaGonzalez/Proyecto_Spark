# -*- coding: utf-8 -*-
"""3.3 Top 10 most profitable directors. .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FeHvPr6rTc9veu6KS0wAUwszU5z8Vt7R
"""

!pip install gdown

import gdown

# Define the URL of the file to download
url = 'https://drive.google.com/uc?id=12W6v3c0daCXCwtRK100j0XuEe0_b77Dv'

# Define the output file path and name
output = '/content/dataset.zip'

# Download the file
gdown.download(url, output, quiet=False)

import pandas as pd
import zipfile

# Specify the path to the downloaded ZIP file
zip_file_path = '/content/dataset.zip'

# Extract the contents of the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall('/content/')

!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
!python get-pip.py

!pip install pyspark

from pyspark.sql.functions import col, from_json, when
from pyspark.sql.types import StructType

# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Read the movies dataset into a Spark DataFrame

import json


creditss = pd.read_csv('/content/tmdb_5000_credits.csv')
movies = pd.read_csv('/content/tmdb_5000_movies.csv')

import pyspark.sql.functions as F
full = pd.merge(movies, creditss, left_on='id', right_on='movie_id', how='left')

full.drop(['homepage','original_title','overview','spoken_languages',
           'status','tagline','movie_id'],axis=1,inplace=True)

full.isnull().any()


json_column = ['genres','keywords','production_companies',
               'production_countries','cast','crew']

for column in json_column:
    full[column]=full[column].map(json.loads)

def getname(x):
    list = []
    for i in x:
        list.append(i['name'])
    return ','.join(list)

for column in json_column[0:4]:
    full[column] = full[column].map(getname)

def getcharacter(x):
    list = []
    for i in x:
        list.append(i['character'])
    return ','.join(list[0:2])


full['cast']=full['cast'].map(getcharacter)

def getdirector(x):
    list=[]
    for i in x:
        if i['job']=='Director':
            list.append(i['name'])
    return ",".join(list)

full['crew']=full['crew'].map(getdirector)

def get_actors(x):
    actor_list = x.split(',')
    return ','.join(actor_list)

full['actors'] = full['cast'].apply(get_actors)

rename_dict = {'release_date':'year','cast':'actor','crew':'director'}
full.rename(columns=rename_dict, inplace=True)

original_df = full.copy()

original_df.head()

spark_df = spark.createDataFrame(original_df)
spark_df.show()

spark_df = spark_df.withColumn('budget', spark_df['budget'].cast('double'))
spark_df = spark_df.withColumn('revenue', spark_df['revenue'].cast('double'))

spark_df = spark_df.withColumn('profit', spark_df['revenue'] - spark_df['budget'])

director_profits = spark_df.groupBy('director').sum('profit').withColumnRenamed('sum(profit)', 'total_profit')

sorted_directors = director_profits.orderBy('total_profit', ascending=False)

top_10_directors = sorted_directors.limit(10)

top_10_directors.show()