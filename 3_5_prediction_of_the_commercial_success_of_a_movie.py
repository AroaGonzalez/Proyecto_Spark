# -*- coding: utf-8 -*-
"""3.5. Prediction of the commercial success of a movie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/121YfKKyLXZv66y0x6iAgKa4T0XP1m9Ve
"""

!pip install gdown

import gdown

# Define the URL of the file to download
url = 'https://drive.google.com/uc?id=12W6v3c0daCXCwtRK100j0XuEe0_b77Dv'

# Define the output file path and name
output = '/content/dataset.zip'

# Download the file
gdown.download(url, output, quiet=False)

import pandas as pd
import zipfile

# Specify the path to the downloaded ZIP file
zip_file_path = '/content/dataset.zip'

# Extract the contents of the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall('/content/')

!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
!python get-pip.py

!pip install pyspark

from pyspark.sql.functions import col, from_json, when
from pyspark.sql.types import StructType

# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Read the movies dataset into a Spark DataFrame

import json


creditss = pd.read_csv('/content/tmdb_5000_credits.csv')
movies = pd.read_csv('/content/tmdb_5000_movies.csv')

import pyspark.sql.functions as F
full = pd.merge(movies, creditss, left_on='id', right_on='movie_id', how='left')

full.drop(['homepage','original_title','overview','spoken_languages',
           'status','tagline','movie_id'],axis=1,inplace=True)

full.isnull().any()


json_column = ['genres','keywords','production_companies',
               'production_countries','cast','crew']

for column in json_column:
    full[column]=full[column].map(json.loads)

def getname(x):
    list = []
    for i in x:
        list.append(i['name'])
    return ','.join(list)

for column in json_column[0:4]:
    full[column] = full[column].map(getname)

def getcharacter(x):
    list = []
    for i in x:
        list.append(i['character'])
    return ','.join(list[0:2])


full['cast']=full['cast'].map(getcharacter)

def getdirector(x):
    list=[]
    for i in x:
        if i['job']=='Director':
            list.append(i['name'])
    return ",".join(list)

full['crew']=full['crew'].map(getdirector)

def get_actors(x):
    actor_list = x.split(',')
    return ','.join(actor_list)

full['actors'] = full['cast'].apply(get_actors)

rename_dict = {'release_date':'year','cast':'actor','crew':'director'}
full.rename(columns=rename_dict, inplace=True)

original_df = full.copy()

original_df.head()

spark_df = spark.createDataFrame(original_df)
spark_df.show()

spark_df = spark_df.withColumn('success', when(col('revenue') - col('budget')>0, 1).otherwise(0))
spark_df.show()

a = spark_df.filter(spark_df['success']==0).count()
b = spark_df.filter(spark_df['success']==1).count()
print(f"Amount of 0: {a}")
print(f"Amount of 1: {b}")

spark_df2 = spark_df.na.drop()

spark_df2 = spark_df.withColumn('budget', spark_df['budget'].cast('double'))
spark_df2 = spark_df.withColumn('popularity', spark_df['popularity'].cast('double'))
spark_df2 = spark_df.withColumn('runtime', spark_df['runtime'].cast('double'))
spark_df2 = spark_df.withColumn('vote_average', spark_df['vote_average'].cast('double'))
spark_df2 = spark_df.withColumn('vote_count', spark_df['vote_count'].cast('integer'))

#Logistic regresion

from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
 
(train_d, test_d) = spark_df.randomSplit([0.8, 0.2], seed = 42)

tokenizer = Tokenizer(inputCol='keywords', outputCol='words')
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')
lr= LogisticRegression(maxIter=10, labelCol = 'success')
pipeline = Pipeline(stages = [tokenizer, hashingTF, lr])

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder

paramGrid = ParamGridBuilder() \
      .addGrid(hashingTF.numFeatures, [100, 1000, 10000]) \
      .addGrid(lr.regParam, [0.1, 0.01]) \
      .build()

from pyspark.ml.tuning import CrossValidator

# Crear un objeto CrossValidator
crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(labelCol='success'),
                          numFolds=3)

# Ajustar el modelo utilizando CrossValidator
cvModel = crossval.fit(train_d)

predictions = cvModel.transform(test_d)

evaluator = BinaryClassificationEvaluator(labelCol='success')

test_auc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})

print('AUC. {:.2f}'.format(test_auc))

#Random Forest

from pyspark.ml.classification import RandomForestClassifier

(train_d, test_d) = spark_df.randomSplit([0.9, 0.1], seed=42)

tokenizer_rf = Tokenizer(inputCol='keywords', outputCol='words')
hashingTF_rf = HashingTF(inputCol=tokenizer_rf.getOutputCol(), outputCol='features')

rf = RandomForestClassifier(labelCol='success')

pipeline_rf = Pipeline(stages=[tokenizer_rf, hashingTF_rf, rf])

paramGrid_rf = ParamGridBuilder() \
    .addGrid(hashingTF_rf.numFeatures, [100, 1000, 10000]) \
    .addGrid(rf.numTrees, [10, 50, 100]) \
    .build()

crossval_rf = CrossValidator(estimator=pipeline_rf,
                          estimatorParamMaps=paramGrid_rf,
                          evaluator=BinaryClassificationEvaluator(labelCol='success'),
                          numFolds=3)

cvModel_rf = crossval_rf.fit(train_d)

predictions_rf = cvModel_rf.transform(test_d)

evaluator_rf = BinaryClassificationEvaluator(labelCol='success')

test_auc_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: 'areaUnderROC'})
print('AUC. {:.2f}'.format(test_auc_rf))

#Support Vector Machine (SVM)

from pyspark.ml.classification import LinearSVC

(train_d, test_d) = spark_df.randomSplit([0.9, 0.1], seed=42)

tokenizer_svm = Tokenizer(inputCol='keywords', outputCol='words')
hashingTF_svm = HashingTF(inputCol=tokenizer_svm.getOutputCol(), outputCol='features')

svm = LinearSVC(labelCol='success')

pipeline_svm = Pipeline(stages=[tokenizer_svm, hashingTF_svm, svm])

paramGrid_svm = ParamGridBuilder() \
    .addGrid(hashingTF_svm.numFeatures, [100, 1000, 10000]) \
    .addGrid(svm.regParam, [0.1, 0.01]) \
    .build()

crossval_svm = CrossValidator(estimator=pipeline_svm,
                              estimatorParamMaps=paramGrid_svm,
                              evaluator=BinaryClassificationEvaluator(labelCol='success'),
                              numFolds=3)

cvModel_svm = crossval_svm.fit(train_d)

predictions_svm = cvModel_svm.transform(test_d)

evaluator_svm = BinaryClassificationEvaluator(labelCol='success')

test_auc_svm = evaluator_svm.evaluate(predictions_svm, {evaluator_svm.metricName: 'areaUnderROC'})
print('AUC. {:.2f}'.format(test_auc_svm))