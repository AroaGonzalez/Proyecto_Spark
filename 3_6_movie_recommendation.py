# -*- coding: utf-8 -*-
"""3.6. Movie recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h4RDVNvn7UsDz-_xlbTHX7MOV1zoio3V
"""

!pip install gdown

import gdown

# Define the URL of the file to download
url = 'https://drive.google.com/uc?id=12W6v3c0daCXCwtRK100j0XuEe0_b77Dv'

# Define the output file path and name
output = '/content/dataset.zip'

# Download the file
gdown.download(url, output, quiet=False)

import pandas as pd
import zipfile

# Specify the path to the downloaded ZIP file
zip_file_path = '/content/dataset.zip'

# Extract the contents of the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall('/content/')

!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
!python get-pip.py

!pip install pyspark

from pyspark.sql.functions import col, from_json, when
from pyspark.sql.types import StructType

# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Read the movies dataset into a Spark DataFrame

import json


creditss = pd.read_csv('/content/tmdb_5000_credits.csv')
movies = pd.read_csv('/content/tmdb_5000_movies.csv')

import pyspark.sql.functions as F
full = pd.merge(movies, creditss, left_on='id', right_on='movie_id', how='left')

full.drop(['homepage','original_title','overview','spoken_languages',
           'status','tagline','movie_id'],axis=1,inplace=True)

full.isnull().any()


json_column = ['genres','keywords','production_companies',
               'production_countries','cast','crew']

for column in json_column:
    full[column]=full[column].map(json.loads)

def getname(x):
    list = []
    for i in x:
        list.append(i['name'])
    return ','.join(list)

for column in json_column[0:4]:
    full[column] = full[column].map(getname)

def getcharacter(x):
    list = []
    for i in x:
        list.append(i['character'])
    return ','.join(list[0:2])


full['cast']=full['cast'].map(getcharacter)

def getdirector(x):
    list=[]
    for i in x:
        if i['job']=='Director':
            list.append(i['name'])
    return ",".join(list)

full['crew']=full['crew'].map(getdirector)

def get_actors(x):
    actor_list = x.split(',')
    return ','.join(actor_list)

full['actors'] = full['cast'].apply(get_actors)

rename_dict = {'release_date':'year','cast':'actor','crew':'director'}
full.rename(columns=rename_dict, inplace=True)

original_df = full.copy()

original_df.head()

spark_df = spark.createDataFrame(original_df)
spark_df.show()

from pyspark.ml.feature import HashingTF, IDF, Tokenizer, FeatureHasher

tokenizer = Tokenizer(inputCol='keywords', outputCol='words')
spark_df2 = tokenizer.transform(spark_df)

hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=10)


hashedData = hashingTF.transform(spark_df2)

idf = IDF(inputCol="rawFeatures", outputCol="features")
idfModel = idf.fit(hashedData)
idfData = idfModel.transform(hashedData)

idfData.select('id', 'features').show(truncate=False)

import numpy as np

result = idfData.select("id", "features").rdd.map(lambda x: (x[0], x[1].toArray())).collect()

from scipy.spatial import distance

def cos_dis(id, result):
  distances = []
  arr= None
  for i in result:
    if int(id) == i[0]:
      arr = i[1]
    
  for z in result:
    r = distance.cosine(arr, z[1])
    distances.append((z[0], r))
  return distances

id_number=767

result = cos_dis(id_number, result)
source = spark_df2.filter(spark_df2["id"] == str(id_number)).collect()[0]

# Put the results of HashingTF vectorization at hash_tf_result variable in the following form: [(article_id, score), ...]
hash_tf_result =  sorted(result, key=lambda x:x[1])[:6] #6 because one of these will be the actual source film

print(f'Top 5 results according HashingTF vectorization for article {source.id} - {source.title_x}:')
for item in hash_tf_result:
  if item[0] != source.id:
    rec = spark_df2.filter(spark_df2.id == item[0]).first()
    print(f'{rec.id} - {rec.title_x} ({item[1]})')